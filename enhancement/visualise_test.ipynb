{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of test images with a given input model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a path to the trained UNet model, the script visualises 5 test images predicted by the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "\n",
    "import torch\n",
    "import models\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from patchify import patchify, unpatchify\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.exposure import equalize_hist\n",
    "import math\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from torchvision.transforms import Resize\n",
    "import torch.nn as nn\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "def image_transform(img, amp=False, raw=False, output=False, crop = True):\n",
    "    if raw and not output:\n",
    "        arr = np.fromfile(open(img, 'rb'), dtype=np.uint8).reshape(2160,4096)\n",
    "        arr = np.repeat(np.expand_dims(arr, axis=2), 3, axis=2)\n",
    "    else:\n",
    "        img = cv2.imread(img, -1)\n",
    "\n",
    "    if crop:\n",
    "        # print(img.shape, 'img shape before cropping')\n",
    "        img = img[:2048, :, :]\n",
    "    # if output:\n",
    "        # print(arr, 'Output array printed')\n",
    "    if output == False:\n",
    "        # print('Input')\n",
    "        img = torch.from_numpy(img / 255).permute(2, 0, 1).unsqueeze(0)\n",
    "        if amp:\n",
    "        # print(arr.shape, 'Shape in here')\n",
    "        # amp = amp_module(arr, 0.5)\n",
    "            img = (img*amp).clamp_(0,1)\n",
    "        # arr = ()\n",
    "    else:\n",
    "        # print('Output')\n",
    "        img = torch.from_numpy(img/255).permute(2, 0, 1).unsqueeze(0)\n",
    "    # res, h, w, p_x, p_y = patchify_img(np.array(arr), patch_size=512)\n",
    "    # arr = torch.from_numpy(res).permute(0, 3, 1, 2)\n",
    "    # if amp:\n",
    "        # return arr, h, w, p_x, p_y, amp, amp_arr\n",
    "    # else:\n",
    "        # return arr, h, w, p_x, p_y\n",
    "    # print(arr.shape, 'arr shape')\n",
    "    # print(img.shape, 'img.shape')\n",
    "    return img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:1')\n",
    "torch.cuda.device_count()\n",
    "dir = '/home/gpu/girish/results_clrimg_resized/_unet_basic2_100_new_amp_100'\n",
    "amp = 100\n",
    "split_ratio = 0\n",
    "raw = False\n",
    "monochrome= False\n",
    "crop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model construction\n",
    "model_path = os.path.join(dir, 'epoch-best-psnr.pth')\n",
    "sv_file = torch.load(model_path, map_location=device)\n",
    "model = models.make(sv_file['model'], load_sd=True).eval().cuda()\n",
    "model = nn.DataParallel(model, device_ids=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_weights_path = os.path.join(dir, \"epoch-best-psnr.pth\")\n",
    "# sv_file = torch.load(model_weights_path)\n",
    "# model = models.make(sv_file[\"model\"], load_sd=True).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #! Patichyfies ? \n",
    "\n",
    "# def patchify_img(image, patch_size=256):\n",
    "#     size_x = (image.shape[0] // patch_size) * patch_size  # get width to nearest size divisible by patch size\n",
    "#     size_y = (image.shape[1] // patch_size) * patch_size\n",
    "#     instances = []\n",
    "\n",
    "#     # Crop original image to size divisible by patch size from top left corner\n",
    "#     image = image[:size_x, :size_y, :]\n",
    "\n",
    "#     # Extract patches from each image, step=patch_size means no overlap\n",
    "#     patch_img = patchify(image, (patch_size, patch_size, 3), step=patch_size)\n",
    "\n",
    "#     # iterate over vertical patch axis\n",
    "#     for j in range(patch_img.shape[0]):\n",
    "#         # iterate over horizontal patch axis\n",
    "#         for k in range(patch_img.shape[1]):\n",
    "#             # patches are located like a grid. use (j, k) indices to extract single patched image\n",
    "#             single_patch_img = patch_img[j, k]\n",
    "\n",
    "#             # Drop extra extra dimension from patchify\n",
    "#             instances.append(np.squeeze(single_patch_img))\n",
    "#     return np.vstack([np.expand_dims(x, 0) for x in instances]), size_x, size_y, patch_img.shape[0], patch_img.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def amp_module(img, m):\n",
    "#   \"\"\"\n",
    "#   img.shape - (3, H, W)\n",
    "#   m - central value (typically 0.5) \n",
    "#   \"\"\"\n",
    "#   # H = img.shape[0]\n",
    "#   W = img.shape[1]\n",
    "#   sumOfPixels = torch.sum(img)\n",
    "#   amp = m*(3*H*W)/(sumOfPixels)\n",
    "#   print(amp)\n",
    "#   return amp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def image_transform(img, amp=False, raw=False, output=False):\n",
    "#     if raw and not output:\n",
    "#         arr = np.fromfile(open(img, 'rb'), dtype=np.uint8).reshape(2160,4096)\n",
    "#         arr = np.repeat(np.expand_dims(arr, axis=2), 3, axis=2)\n",
    "#     else:\n",
    "#         arr = np.array(Image.open(img))\n",
    "#         if len(arr.shape)==3:\n",
    "#             pass\n",
    "#         elif len(arr.shape)==2:\n",
    "#             arr = np.repeat(np.expand_dims(arr, axis=2), 3, axis=2)\n",
    "#     res, h, w, p_x, p_y = patchify_img(np.array(arr), patch_size=512)\n",
    "#     arr = torch.from_numpy(res/255).permute(0, 3, 1, 2)\n",
    "#     if amp:\n",
    "#         arr = (arr*amp).clamp_(0,1)\n",
    "#     return arr, h, w, p_x, p_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load image - Probably the main function everyhting seems to happen - 18secs to run\n",
    "\n",
    "# # Original\n",
    "\n",
    "# # Train Config is being loaded here \n",
    "# with open(os.path.join(dir, 'config.yaml'), 'r') as f:\n",
    "#         config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        \n",
    "# if config['val_dataset']['dataset']['args'].get('root_path_inp') is not None:\n",
    "#     test_dir = config['val_dataset']['dataset']['args']['root_path_inp']\n",
    "#     out_dir = config['val_dataset']['dataset']['args']['root_path_out']\n",
    "#     filenames = sorted(os.listdir(test_dir))\n",
    "#     outfile = sorted(os.listdir(out_dir))\n",
    "#     img_files = filenames[math.ceil(len(filenames)*split_ratio):]\n",
    "#     print(len(img_files), len(filenames))\n",
    "#     #img_files = filenames[0:]\n",
    "#     imgs, h_s, w_s, pxs, pys, out_imgs = [], [], [], [], [], []\n",
    "#     for file in img_files:\n",
    "#         res = image_transform(os.path.join(test_dir,file), amp, raw)\n",
    "#         if monochrome:\n",
    "#             imgs.append(res[0][:,:1,:,:])\n",
    "#         else:\n",
    "#             imgs.append(res[0])\n",
    "#         h_s.append(res[1])\n",
    "#         w_s.append(res[2])\n",
    "#         pxs.append(res[3])\n",
    "#         pys.append(res[4])\n",
    "#         out_imgs.append(image_transform(os.path.join(out_dir, file), raw=raw, output=True)[0])\n",
    "                       \n",
    "# else:\n",
    "#     test_dir1 = config['val_dataset']['dataset']['args']['root_path_inp1']\n",
    "#     test_dir2 = config['val_dataset']['dataset']['args']['root_path_inp2']\n",
    "#     out_dir = config['val_dataset']['dataset']['args']['root_path_out']\n",
    "#     filenames = sorted(os.listdir(test_dir1))\n",
    "#     img_files = filenames[math.ceil(len(filenames)*split_ratio):]\n",
    "#     outfile = sorted(os.listdir(out_dir))\n",
    "#     imgs, h_s, w_s, pxs, pys, out_imgs = [], [], [], [], [], []\n",
    "    \n",
    "#     for file in img_files:\n",
    "#         res1 = image_transform(os.path.join(test_dir1, file), amp, raw)\n",
    "#         res2 = image_transform(os.path.join(test_dir2, file), amp, raw)\n",
    "    \n",
    "#         if monochrome:\n",
    "#             res = torch.cat([res1[0][:,:1,:,:], res2[0][:,:1,:,:]], axis=1)\n",
    "#             imgs.append(res)\n",
    "#         else:\n",
    "#             res = torch.cat([res1[0], res2[0]], dim=1)\n",
    "#             imgs.append(res)\n",
    "#         h_s.append(res2[1])\n",
    "#         w_s.append(res2[2])\n",
    "#         pxs.append(res2[3])\n",
    "#         pys.append(res2[4])\n",
    "#         out_imgs.append(image_transform(os.path.join(out_dir, file), raw=raw, output=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load image - Probably the main function everyhting seems to happen - 18secs to run\n",
    "\n",
    "# This is for the TEST Set\n",
    "\n",
    "# Train Config is being loaded here \n",
    "amp_imgs = []\n",
    "avg_amp = []\n",
    "with open(os.path.join(dir, 'test_config.yaml'), 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        \n",
    "if config['test_dataset']['dataset']['args'].get('root_path_inp') is not None:\n",
    "    test_dir = config['test_dataset']['dataset']['args']['root_path_inp']\n",
    "    out_dir = config['test_dataset']['dataset']['args']['root_path_out']\n",
    "    filenames = sorted(os.listdir(test_dir))\n",
    "    outfile = sorted(os.listdir(out_dir))\n",
    "    img_files = filenames[math.ceil(len(filenames)*split_ratio):]\n",
    "    print(len(img_files), len(filenames))\n",
    "    # img_files = ['68.png', '162.png', '134.png', '173.png', '18.png', '184.png'] #! filenames\n",
    "    imgs, h_s, w_s, pxs, pys, out_imgs = [], [], [], [], [], []\n",
    "    for file in img_files[:1]:\n",
    "        # print('Input Transformation started')\n",
    "        res = image_transform(os.path.join(test_dir,file), amp = amp, raw = raw, crop = crop, output=False)\n",
    "        # print(res.shape, 'Image after transformation')\n",
    "        if monochrome:\n",
    "            imgs.append(res[0][:,:1,:,:])\n",
    "        else:\n",
    "            imgs.append(res)\n",
    "            print(res.shape, 'Input shape res.shape')\n",
    "        # h_s.append(res[1])\n",
    "        # w_s.append(res[2])\n",
    "        # pxs.append(res[3])\n",
    "        # pys.append(res[4])\n",
    "        # if amp:\n",
    "            # amp_imgs.append(res[6])\n",
    "            # avg_amp.append(res[5])\n",
    "        # out_imgs.append(image_transform(os.path.join(out_dir, file), raw=raw, output=True)[0])\n",
    "        # print('Output transformation started')\n",
    "        out_imgs.append(image_transform(os.path.join(out_dir, file), raw=raw, output=True))\n",
    "                       \n",
    "else:\n",
    "    test_dir1 = config['test_dataset']['dataset']['args']['root_path_inp1']\n",
    "    test_dir2 = config['test_dataset']['dataset']['args']['root_path_inp2']\n",
    "    out_dir = config['test_dataset']['dataset']['args']['root_path_out']\n",
    "    filenames = sorted(os.listdir(test_dir1))\n",
    "    img_files = filenames[math.ceil(len(filenames)*split_ratio):]\n",
    "    outfile = sorted(os.listdir(out_dir))\n",
    "    imgs, h_s, w_s, pxs, pys, out_imgs = [], [], [], [], [], []\n",
    "    \n",
    "    for file in img_files:\n",
    "        res1 = image_transform(os.path.join(test_dir1, file), amp, raw)\n",
    "        res2 = image_transform(os.path.join(test_dir2, file), amp, raw)\n",
    "    \n",
    "        if monochrome:\n",
    "            res = torch.cat([res1[0][:,:1,:,:], res2[0][:,:1,:,:]], axis=1)\n",
    "            imgs.append(res)\n",
    "        else:\n",
    "            res = torch.cat([res1[0], res2[0]], dim=1)\n",
    "            imgs.append(res)\n",
    "        h_s.append(res2[1])\n",
    "        w_s.append(res2[2])\n",
    "        pxs.append(res2[3])\n",
    "        pys.append(res2[4])\n",
    "        out_imgs.append(image_transform(os.path.join(out_dir, file), raw=raw, output=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load image - Probably the main function everyhting seems to happen - 18secs to run\n",
    "\n",
    "\n",
    "\n",
    "# # Train Config is being loaded here \n",
    "# with open(os.path.join(dir, 'config.yaml'), 'r') as f:\n",
    "#         config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        \n",
    "# if config['val_dataset']['dataset']['args'].get('root_path_inp') is not None:\n",
    "#     test_dir = config['val_dataset']['dataset']['args']['root_path_inp']\n",
    "#     out_dir = config['val_dataset']['dataset']['args']['root_path_out']\n",
    "#     filenames = sorted(os.listdir(test_dir))\n",
    "#     outfile = sorted(os.listdir(out_dir))\n",
    "#     img_files = filenames\n",
    "#     print(len(img_files), len(filenames))\n",
    "#     #img_files = filenames[0:]\n",
    "#     imgs, h_s, w_s, pxs, pys, out_imgs = [], [], [], [], [], []\n",
    "#     for file in img_files:\n",
    "#         # Image_Transform is happening. Need to check if this is the same transformation happening with testing as well \n",
    "#         res = image_transform(os.path.join(test_dir,file), amp, raw)\n",
    "#         if monochrome:\n",
    "#             imgs.append(res[0][:,:1,:,:])\n",
    "#         else:\n",
    "#             imgs.append(res[0])\n",
    "#         h_s.append(res[1])\n",
    "#         w_s.append(res[2])\n",
    "#         pxs.append(res[3])\n",
    "#         pys.append(res[4])\n",
    "#         out_imgs.append(image_transform(os.path.join(out_dir, file), raw=raw, output=True)[0])\n",
    "                       \n",
    "# else:\n",
    "#     test_dir1 = config['val_dataset']['dataset']['args']['root_path_inp1']\n",
    "#     test_dir2 = config['val_dataset']['dataset']['args']['root_path_inp2']\n",
    "#     out_dir = config['val_dataset']['dataset']['args']['root_path_out']\n",
    "#     filenames = sorted(os.listdir(test_dir1))\n",
    "#     img_files = filenames[math.ceil(len(filenames)*split_ratio):]\n",
    "#     outfile = sorted(os.listdir(out_dir))\n",
    "#     imgs, h_s, w_s, pxs, pys, out_imgs = [], [], [], [], [], []\n",
    "    \n",
    "#     for file in img_files:\n",
    "#         res1 = image_transform(os.path.join(test_dir1, file), amp, raw)\n",
    "#         res2 = image_transform(os.path.join(test_dir2, file), amp, raw)\n",
    "    \n",
    "#         if monochrome:\n",
    "#             res = torch.cat([res1[0][:,:1,:,:], res2[0][:,:1,:,:]], axis=1)\n",
    "#             imgs.append(res)\n",
    "#         else:\n",
    "#             res = torch.cat([res1[0], res2[0]], dim=1)\n",
    "#             imgs.append(res)\n",
    "#         h_s.append(res2[1])\n",
    "#         w_s.append(res2[2])\n",
    "#         pxs.append(res2[3])\n",
    "#         pys.append(res2[4])\n",
    "#         out_imgs.append(image_transform(os.path.join(out_dir, file), raw=raw, output=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(pred, out, rgb_range=1):\n",
    "    '''\n",
    "    inp: patch_count * channels * H * W\n",
    "    pred: patch_count * channels * H * W\n",
    "    '''\n",
    "    diff = (pred - out)/ rgb_range\n",
    "    mse = torch.mean(torch.pow(diff, 2))\n",
    "\n",
    "    return -10 * torch.log10(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img_files)):\n",
    "    file_name = img_files[i]\n",
    "    img = imgs[i] # 1 x 3 x 2048 x 4096, torch.float64\n",
    "    # RESIZING\n",
    "    img = img.squeeze(dim = 0) # 3 x 2048 x 4096, torch.float64\n",
    "    resizer = Resize([1024, 2048])\n",
    "    img = resizer(img) # 3 x 1024 x 2048, torch.float64\n",
    "    img = img.float() # 3 x 1024 x 2048, torch.float32\n",
    "    # UNSQUEEZING\n",
    "    img = img.unsqueeze(dim = 0) # 1 x 3 x 1024 x 2048, torch.float32\n",
    "    print(img.dtype)\n",
    "    print(img.shape)\n",
    "    with torch.no_grad():\n",
    "        pred = model(((img - 0.5)/0.5))*0.5 + 0.5\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out_imgs)\n",
    "print(out_imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_arr = []\n",
    "ssim_arr = []\n",
    "for i in range(len(img_files)):\n",
    "\n",
    "    # Setting up the image\n",
    "    resizer = Resize([1024, 2048])\n",
    "    file_name = img_files[i]\n",
    "\n",
    "    # Resizing the GT\n",
    "    gt = out_imgs[i]\n",
    "    gt = torch.reshape(gt, (3,2048, 4096))\n",
    "    gt = resizer(gt)\n",
    "\n",
    "\n",
    "    # Prediction\n",
    "    img = imgs[i] # 1 x 3 x 2048 x 4096, torch.float64\n",
    "    # RESIZING\n",
    "    img = img.squeeze(dim = 0) # 3 x 2048 x 4096, torch.float64\n",
    "    resizer = Resize([1024, 2048])\n",
    "    img = resizer(img) # 3 x 1024 x 2048, torch.float64\n",
    "    img = img.float() # 3 x 1024 x 2048, torch.float32\n",
    "    # UNSQUEEZING\n",
    "    img = img.unsqueeze(dim = 0) # 1 x 3 x 1024 x 2048, torch.float32\n",
    "    print(img.dtype)\n",
    "    print(img.shape)\n",
    "    with torch.no_grad():\n",
    "        pred = model(((img - 0.5)/0.5))*0.5 + 0.5 \n",
    "    temp = psnr(pred.cpu(), gt).item()\n",
    "    print(temp)\n",
    "    psnr_arr.append(temp)\n",
    "\n",
    "\n",
    "    # Ground truth Display\n",
    "    plt.figure(figsize=(20,12))\n",
    "    plt.subplot(2,2,1)\n",
    "    gt = torch.permute(gt, (1, 2 , 0)).detach().cpu().numpy()\n",
    "    gt = gt[:, :, [2,1,0]]\n",
    "    plt.title('Ground Truth {}'.format(file_name))\n",
    "    plt.imshow(gt, 'gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,2,2)\n",
    "\n",
    "    pred = torch.permute(pred, (1,2,0)).detach().cpu().numpy\n",
    "    print(pred)\n",
    "    plt.title('Output')\n",
    "    plt.imshow(pred, 'gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    ssim_arr.append(ssim(pred, gt.astype(np.float32()), channel_axis=2))\n",
    "    plt.subplot(2,2,3)\n",
    "    inp = torch.permute(inp, (1,2,0)).detach().cpu().numpy()\n",
    "    plt.title('Amplifid Image')\n",
    "    plt.imshow(inp, 'gray')\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.title('Difference Between Output and Input')\n",
    "    plt.imshow(abs(gt-pred), 'gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.01, hspace=0.1)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predict\n",
    "# results, psnrs, ssims = [], [], []\n",
    "# print(len(imgs))\n",
    "# for i, img in enumerate(imgs):\n",
    "#     pred_patches = []\n",
    "#     # print(img.shape)\n",
    "#     for patch in img:\n",
    "#         pred = model((patch.unsqueeze(0).float()-0.5)/0.5)\n",
    "#         pred_patches.append((pred*0.5+0.5).clamp_(0,1).detach().cpu())\n",
    "#     pred = torch.vstack(pred_patches)\n",
    "#     psnrs.append(psnr(pred, out_imgs[i][:,:3,:,:]).item())\n",
    "#     result = unpatchify(pred.permute(0,2,3,1).reshape(pxs[0],pys[0],1,512,512,3).detach().numpy(), (h_s[0],w_s[0],3))\n",
    "#     results.append(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_ = []\n",
    "# for i in range(len(results)):\n",
    "#     if out_imgs[i].shape[0] == 28: #! Kinda try to generalize not plain 28 \n",
    "#         gt = unpatchify(out_imgs[i][:,:3,:,:].permute(0,2,3,1).reshape(4,7,1,512,512,3).detach().numpy(), (h_s[0],3584,3))\n",
    "#     else:\n",
    "#         gt = unpatchify(out_imgs[i][:,:3,:,:].permute(0,2,3,1).reshape(pxs[0],pys[0],1,512,512,3).detach().numpy(), (h_s[0],w_s[0],3))\n",
    "#     ssims.append(ssim(results[i], gt, channel_axis=2))\n",
    "#     gt_.append(gt)\n",
    "    \n",
    "# avg_ssim = np.mean(ssims)\n",
    "# print(avg_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #visualise\n",
    "# sorted_psnrs = sorted(psnrs)\n",
    "\n",
    "# for psnr in sorted_psnrs[:5]:\n",
    "#     index = psnrs.index(psnr)\n",
    "#     plt.figure(figsize=(20,12))\n",
    "#     plt.subplot(2,2,1)\n",
    "#     if raw:\n",
    "#         plt.imshow(np.fromfile(open(os.path.join(out_dir, img_files1[index]), 'rb'), dtype=np.uint8).reshape(2160,4096), 'gray')\n",
    "#     else:\n",
    "#         # plt.imshow(Image.open(os.path.join(out_dir, img_files[index])), 'gray')\n",
    "#         plt.imshow(gt_[index], 'gray')\n",
    "#     plt.title('Ground Truth - {}'.format(img_files[index]))\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     plt.subplot(2,2,2)\n",
    "#     plt.imshow(results[index], 'gray')\n",
    "#     plt.title('PSNR: '+str(round(psnr,2)) + ' SSIM: '+str(round(ssims[index], 2)))\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     plt.subplot(2,2,3)\n",
    "#     # plt.imshow(Image.open(os.path.join(out_dir, img_files[index])), 'gray')\n",
    "#     plt.imshow(abs(gt_[index] - results[index]),'gray')\n",
    "#     plt.title('Difference')\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     plt.subplot(2,2,4)\n",
    "#     plt.imshow(amp_imgs[index], 'gray')\n",
    "#     plt.title('Amplified Input Image with amp = {}'.format(int(avg_amp[index])))\n",
    "#     plt.axis('off')\n",
    "#     plt.subplots_adjust(wspace =0.01, hspace = 0.000001)\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #visualise\n",
    "# sorted_psnrs = sorted(psnrs)\n",
    "\n",
    "# for psnr in sorted_psnrs[-5:]:\n",
    "#     index = psnrs.index(psnr)\n",
    "#     plt.figure(figsize=(20,12))\n",
    "#     plt.subplot(2,2,1)\n",
    "#     if raw:\n",
    "#         plt.imshow(np.fromfile(open(os.path.join(out_dir, img_files1[index]), 'rb'), dtype=np.uint8).reshape(2160,4096), 'gray')\n",
    "#     else:\n",
    "#         # plt.imshow(Image.open(os.path.join(out_dir, img_files[index])), 'gray')\n",
    "#         plt.imshow(gt_[index], 'gray')\n",
    "#     plt.title('Ground Truth - {}'.format(img_files[index]))\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     plt.subplot(2,2,2)\n",
    "#     plt.imshow(results[index], 'gray')\n",
    "#     plt.title('PSNR: '+str(round(psnr,2)) + ' SSIM: '+str(round(ssims[index], 2)))\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     plt.subplot(2,2,3)\n",
    "#     # plt.imshow(Image.open(os.path.join(out_dir, img_files[index])), 'gray')\n",
    "#     plt.imshow(abs(gt_[index] - results[index]),'gray')\n",
    "#     plt.title('Difference')\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     plt.subplot(2,2,4)\n",
    "#     plt.imshow(amp_imgs[index], 'gray')\n",
    "#     plt.title('Amplified Input Image with amp = {}'.format(int(avg_amp[index])))\n",
    "#     plt.axis('off')\n",
    "#     plt.subplots_adjust(wspace =0.01, hspace = 0.000001)\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
